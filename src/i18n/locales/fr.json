{
  "header": {
    "software": "Logiciel",
    "internet": "Internet",
    "tech": "Technologies",
    "kubrick": "1969 Kubrick",
    "watson": "Watson",
    "newEra": "Nouvelle √®re IA",
    "knowyou": "KnowYOU",
    "goodPrompt": "Bon Prompt"
  },
  "hero": {
    "title": "Power by KnowYOU",
    "subtitle": "Transformer la Communication √† l'√àre de l'Intelligence Artificielle",
    "description": "Explorez le parcours de l'√©volution technologique et d√©couvrez comment l'IA r√©volutionne la communication dans le secteur de la sant√© avec KnowYOU.",
    "ctaHealth": "En savoir plus sur la Sant√©",
    "ctaHistory": "Explorer l'histoire de l'IA",
    "ctaHealthTooltip": "Naviguer vers la section interactive KnowYOU pour discuter avec l'assistant de sant√©",
    "ctaHistoryTooltip": "Ouvrir la chronologie interactive montrant l'√©volution de l'Intelligence Artificielle depuis 1950",
    "quote": "Le moment exact o√π nous avons cess√© de simplement op√©rer des machines et avons commenc√©, en fait, √† penser avec elles.",
    "quoteAuthor": "par Fernando Arbache"
  },
  "sections": {
    "software": {
      "title": "La Premi√®re R√©volution",
      "subtitle": "Ann√©es 1940-1980",
      "quote": "Le moment exact o√π nous avons cess√© de simplement op√©rer des machines et avons commenc√©, en fait, √† penser avec elles.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "Le logiciel a repr√©sent√© la premi√®re grande communication entre humains et machines. Gr√¢ce aux langages de programmation, nous avons pu instruire les ordinateurs √† effectuer des t√¢ches complexes. C'√©tait une communication rigide, technique, accessible uniquement aux sp√©cialistes.",
      "content2": "Cette √®re a marqu√© le d√©but de la transformation num√©rique, √©tablissant les bases de toutes les r√©volutions technologiques √† venir."
    },
    "internet": {
      "title": "L'√àre de la Connectivit√©",
      "subtitle": "Ann√©es 1990-2000",
      "quote": "Internet n'a pas seulement raccourci les distances g√©ographiques ; il a entrelac√© le destin de l'humanit√© dans un seul r√©seau pulsant.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "Internet a d√©mocratis√© l'acc√®s √† l'information et transform√© compl√®tement notre fa√ßon de communiquer. Soudain, le monde entier √©tait connect√©, partageant des connaissances √† l'√©chelle mondiale.",
      "content2": "Cette r√©volution n'a pas seulement connect√© des ordinateurs, mais des personnes, des cultures et des id√©es d'une mani√®re jamais imagin√©e auparavant."
    },
    "techNoPropose": {
      "title": "Le Battage Technologique",
      "subtitle": "Ann√©es 2020-2022",
      "quote": "Sans but clair, l'innovation cesse d'√™tre un outil de progr√®s et devient simplement une distraction sophistiqu√©e.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "Toutes les innovations technologiques ne trouvent pas leur but. Le M√©tavers et les NFT promettaient de r√©volutionner le monde num√©rique, mais manquaient d'application pratique et de valeur r√©elle pour les utilisateurs.",
      "content2": "Cette phase nous a enseign√© de pr√©cieuses le√ßons sur la diff√©rence entre l'innovation technologique et l'innovation avec un but r√©el pour la soci√©t√©."
    },
    "kubrick": {
      "title": "La Proph√©tie de Kubrick",
      "subtitle": "1969 - 2001: L'Odyss√©e de l'espace",
      "quote": "Maintenant que la fiction de 2001 est devenue r√©alit√©, le myst√®re n'est plus la capacit√© de la machine, mais l'intention humaine derri√®re elle.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "En 1969, Stanley Kubrick nous a pr√©sent√© HAL 9000, une IA qui non seulement traitait des commandes, mais conversait, comprenait le contexte et avait de la personnalit√©. C'√©tait une pr√©monition impressionnante du futur que nous vivons aujourd'hui.",
      "content2": "Ce qui semblait √™tre de la science-fiction il y a 50 ans est devenu r√©alit√© avec l'av√®nement des grands mod√®les de langage et des assistants IA conversationnels."
    },
    "watson": {
      "title": "Watson: L'√àre de la Cognition",
      "subtitle": "2004 - IBM Watson",
      "quote": "En enseignant √† une machine √† lire les nuances, nous avons d√©couvert que la v√©ritable intelligence n'est pas dans les r√©ponses, mais dans la compr√©hension des questions.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "IBM Watson a marqu√© le d√©but de l'√®re cognitive, d√©montrant que les machines pouvaient non seulement traiter des donn√©es, mais comprendre le langage naturel, raisonner et apprendre.",
      "content2": "Sa victoire √† Jeopardy! en 2011 a √©t√© une √©tape historique, prouvant que l'IA pouvait rivaliser avec les humains dans des t√¢ches n√©cessitant des connaissances larges et une compr√©hension contextuelle."
    },
    "newEra": {
      "title": "La Nouvelle √àre de l'IA",
      "subtitle": "2022 - Pr√©sent",
      "quote": "En d√©mocratisant l'intelligence, nous ne mettons pas seulement √† jour le logiciel du monde, mais r√©√©crivons le potentiel de chaque √™tre humain.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "Avec ChatGPT et des mod√®les similaires, nous sommes entr√©s dans une nouvelle √®re o√π la communication avec les machines est naturelle, fluide et accessible √† tous. La barri√®re technique a √©t√© √©limin√©e.",
      "content2": "Maintenant, n'importe qui peut converser avec une IA comme il le ferait avec un expert humain, ouvrant des possibilit√©s infinies dans l'√©ducation, la sant√©, les affaires et bien plus encore."
    },
    "knowyou": {
      "title": "KnowYOU: Requalification √† l'√àre de l'IA",
      "subtitle": "Technologie Conversationnelle dans la Sant√©",
      "content1": "Le probl√®me de la Longue Tra√Æne r√©v√®le un d√©fi critique: la plupart des gens ne savent pas comment communiquer efficacement avec l'IA. C'est l√† qu'intervient KnowYOU.",
      "content2": "Notre technologie conversationnelle a √©t√© sp√©cialement d√©velopp√©e pour le secteur de la sant√©, rendant la communication avec l'IA naturelle et productive pour tous les professionnels, ind√©pendamment de leurs connaissances techniques."
    },
    "goodPrompt": {
      "title": "L'Art du Bon Prompt",
      "subtitle": "Communication Efficace avec l'IA",
      "quote": "La qualit√© de la r√©ponse d√©pend de la qualit√© de la question.",
      "quoteAuthor": "Fernando Arbache",
      "content1": "Savoir poser les bonnes questions est fondamental √† l'√®re de l'IA. Un bon prompt est clair, sp√©cifique, contextualis√© et dirig√© vers l'objectif souhait√©.",
      "content2": "KnowYOU ne r√©pond pas seulement √† vos questions, mais vous apprend √† mieux communiquer avec l'IA, d√©veloppant une comp√©tence essentielle pour l'avenir du travail en sant√©.",
      "beSpecific": "Soyez Sp√©cifique",
      "beSpecificDesc": "Au lieu de 'parler du diab√®te', demandez 'quels sont les principaux indicateurs pour diagnostiquer le diab√®te de type 2 chez les adultes?'",
      "giveContext": "Fournissez du Contexte",
      "giveContextDesc": "Expliquez votre situation, votre r√¥le et ce que vous devez accomplir avec la r√©ponse.",
      "iterateRefine": "It√©rez et Affinez",
      "iterateRefineDesc": "N'ayez pas peur de poser des questions de suivi pour obtenir exactement les informations dont vous avez besoin."
    }
  },
  "digitalExclusion": {
    "title": "5,74 milliards",
    "subtitle": "de personnes ne peuvent toujours pas acc√©der √† Internet",
    "expandText": "En savoir plus sur ce d√©fi",
    "content1": "Alors que nous c√©l√©brons les progr√®s de l'intelligence artificielle et sa capacit√© √† transformer les industries, la communication et les connaissances, une r√©alit√© troublante demeure: 5,74 milliards de personnes dans le monde n'ont toujours pas acc√®s √† Internet. Cette exclusion num√©rique repr√©sente non seulement une barri√®re technologique, mais un ab√Æme d'opportunit√©s qui s'√©largit exponentiellement √† l'√®re de l'IA g√©n√©rative.",
    "content2": "La d√©mocratisation de l'intelligence artificielle d√©pend fondamentalement de l'acc√®s √† la connectivit√©. Sans Internet, des milliards de personnes sont exclues non seulement des outils d'IA, mais aussi de l'alphab√©tisation n√©cessaire pour comprendre, questionner et utiliser ces technologies de mani√®re critique et productive.",
    "content3": "Au-del√† des barri√®res d'infrastructure, nous faisons face √† des d√©fis linguistiques et cognitifs: la plupart des mod√®les d'IA sont form√©s principalement en anglais, et m√™me lorsqu'ils sont disponibles dans d'autres langues, n√©cessitent un niveau d'alphab√©tisation num√©rique qu'une grande partie de la population mondiale ne poss√®de pas encore. La v√©ritable r√©volution de l'IA ne sera compl√®te que lorsque nous parviendrons √† inclure ces 5,74 milliards de voix dans la conversation."
  },
  "aiHistory": {
    "title": "L'Histoire de l'Intelligence Artificielle",
    "close": "Fermer",
    "timeline": {
      "title": "La Chronologie D√©finitive: De Talos √† la Fi√®vre de l'IA",
      "talos": {
        "title": "Talos, le G√©ant de Bronze",
        "description": "Ce que c'est : Un automate en bronze, cr√©√© par H√©pha√Østos, destin√© √† prot√©ger l'√Æle de Cr√®te.\n\nImpact sur l'IA : Talos est le premier concept philosophique d'une machine autonome. Bien que mythique, il √©tablit l'id√©e humaine de cr√©er un √™tre artificiel avec un objectif sp√©cifique (s√©curit√©/surveillance) qui fonctionne sans intervention humaine constante. Cela pose les bases de la Robotique et des Syst√®mes d'Agents Autonomes, pr√©curseurs des bots et de l'IA op√©rationnelle moderne."
      },
      "telegraphy-cards": {
        "title": "T√©l√©graphie et Cartes Perfor√©es",
        "description": "T√©l√©graphie : La forme la plus ancienne de t√©l√©graphie (le t√©l√©graphe optique de Claude Chappe) est apparue √† la fin du XVIIIe si√®cle (c. 1790). Le t√©l√©graphe √©lectrique, r√©volutionnaire pour la communication et l'encodage des donn√©es, a √©t√© d√©velopp√© par Samuel Morse et Alfred Vail √† partir de 1837.\n\nCartes Perfor√©es : Utilis√©es pour la premi√®re fois dans le M√©tier Jacquard (1801) pour automatiser les motifs de tissage. Herman Hollerith a popularis√© leur utilisation pour le traitement et le stockage de donn√©es lors du recensement am√©ricain de 1890.\n\nImpact sur l'IA : Ont √©tabli les fondements de l'encodage de l'information et du traitement automatis√© des donn√©es, pr√©curseurs directs de l'informatique moderne."
      },
      "turing-machine": {
        "title": "Machine de Turing",
        "description": "Alan Turing √©tablit les fondements th√©oriques de l'informatique, d√©crivant le mod√®le abstrait d'un ordinateur universel capable de r√©soudre tout probl√®me calculable."
      },
      "enigma": {
        "title": "D√©chiffrage du Code Enigma",
        "description": "Alan Turing utilise des machines pr√©curseurs des ordinateurs num√©riques (Bombes) pour d√©chiffrer le cryptage allemand, reliant les machines logiques √† la s√©curit√© de l'information."
      },
      "turing-test": {
        "title": "Test de Turing",
        "description": "Alan Turing propose le 'Jeu de l'imitation', qui devient l'objectif philosophique de l'Intelligence Artificielle: cr√©er des machines capables de converser comme des humains."
      },
      "arpanet": {
        "title": "ARPANET - Premi√®re Connexion",
        "description": "L'embryon d'Internet, utilisant la commutation de paquets pour cr√©er un r√©seau d√©centralis√© et r√©sistant aux pannes. Premier message envoy√© entre ordinateurs."
      },
      "tcpip": {
        "title": "Concept de TCP/IP",
        "description": "Vinton Cerf et Robert Kahn √©tablissent le protocole TCP/IP, le langage fondamental qui permet la communication globale entre r√©seaux h√©t√©rog√®nes."
      },
      "www": {
        "title": "World Wide Web (WWW)",
        "description": "Tim Berners-Lee propose le syst√®me hypertexte (liens), rendant Internet accessible et navigable au grand public via des navigateurs web."
      },
      "social": {
        "title": "Facebook et Orkut - Web 2.0",
        "description": "D√©but du Web 2.0, ax√© sur l'interaction sociale et le contenu g√©n√©r√© par les utilisateurs. Les r√©seaux sociaux transforment notre fa√ßon de communiquer en ligne."
      },
      "watson": {
        "title": "IBM Watson gagne Jeopardy!",
        "description": "Le superordinateur Watson d'IBM bat des champions humains √† Jeopardy!, d√©montrant la capacit√© de l'IA √† traiter le langage naturel et les connaissances complexes en temps r√©el."
      },
      "openai": {
        "title": "Fondation d'OpenAI",
        "description": "Organisation cr√©√©e dans le but de garantir que l'Intelligence Artificielle G√©n√©rale (AGI) profite √† toute l'humanit√©, en privil√©giant la s√©curit√© et l'√©thique."
      },
      "gpt3": {
        "title": "Lancement de GPT-3",
        "description": "Le mod√®le de langage g√©n√®re des textes impossibles √† distinguer de textes √©crits par des humains. R√©volutionne le traitement du langage naturel et stimule la recherche sur les Large Language Models."
      },
      "chatgpt": {
        "title": "Lancement de ChatGPT",
        "description": "L'interface de chat GPT-3.5 devient un ph√©nom√®ne viral. Marque le d√©but de la 'Fi√®vre de l'IA', faisant de l'IA g√©n√©rative un outil de consommation de masse."
      },
      "current": {
        "title": "Web 3.0, Veo et LLMs",
        "description": "Consolidation de l'IA g√©n√©rative (LLMs comme Gemini et GPT-4), outils de cr√©ation vid√©o comme Veo et expansion de l'Internet des Objets. L'IA devient une partie essentielle de l'infrastructure Internet."
      }
    },
    "eras": {
      "dream": {
        "title": "Le R√™ve (Avant 1950)",
        "subtitle": "O√π tout √©tait science-fiction et d√©sir humain.",
        "items": [
          "Antiquit√©: Les mythes grecs parlaient d√©j√† d'automates et de statues qui prenaient vie (comme Talos, le g√©ant de bronze).",
          "1843 - La 'Grand-m√®re' de la Programmation: Ada Lovelace √©crit le premier algorithme pour une machine.",
          "1920 - Le mot 'Robot' na√Æt: Une pi√®ce de th√©√¢tre tch√®que (R.U.R.) utilise le terme pour la premi√®re fois."
        ]
      },
      "birth": {
        "title": "La Naissance (Ann√©es 50)",
        "subtitle": "O√π l'IA gagne un nom et fait ses premiers pas.",
        "items": [
          "1950 - Le Test de Turing: Alan Turing pose la question: 'Les machines peuvent-elles penser?'",
          "1956 - Le Bapt√™me: La Conf√©rence de Dartmouth invente officiellement le terme 'Intelligence Artificielle'.",
          "1957 - Perceptron: Frank Rosenblatt cr√©e une machine qui tente d'imiter un neurone."
        ]
      },
      "childhood": {
        "title": "L'Enfance et l'Adolescence (Ann√©es 60 √† 80)",
        "subtitle": "Hauts et bas, r√©bellion et films hollywoodiens.",
        "items": [
          "1966 - ELIZA, la 'Psychologue': Le premier chatbot de l'histoire!",
          "Ann√©es 70 - L'Hiver de l'IA: Les promesses √©taient trop grandes et la technologie ne tenait pas.",
          "Ann√©es 80 - Le Retour: L'IA revient avec les 'Syst√®mes Experts'.",
          "Terminator (1984): Le cin√©ma cr√©e l'image de l'IA comme m√©chant."
        ]
      },
      "adulthood": {
        "title": "La Phase Adulte (Ann√©es 90 et 2000)",
        "subtitle": "L'IA commence √† battre les humains et √† entrer dans nos maisons.",
        "items": [
          "1997 - √âchec et mat: L'ordinateur Deep Blue (IBM) bat Garry Kasparov aux √©checs.",
          "2002 - Roomba: L'IA entre dans votre salon... pour passer l'aspirateur.",
          "2011 - 'H√©, Siri': Nous commen√ßons √† parler √† nos t√©l√©phones."
        ]
      },
      "revolution": {
        "title": "La R√©volution G√©n√©rative (Ann√©es 2010 √† Aujourd'hui)",
        "subtitle": "L'IA cesse d'analyser seulement et commence √† CR√âER.",
        "items": [
          "2012 - Le 'Big Bang' du Deep Learning: Les r√©seaux neuronaux apprennent √† identifier les chats sur YouTube par eux-m√™mes.",
          "2016 - AlphaGo: L'IA bat le champion mondial de Go avec des coups 'cr√©atifs'.",
          "2017 - Le Transformer: Google publie un article qui change tout.",
          "2022/2023 - L'√àre de ChatGPT et Gemini: L'IA 'sort de sa cage'."
        ]
      }
    }
  },
  "chat": {
    "healthTitle": "KnowYOU",
    "healthSubtitle": "Assistant IA de Sant√©",
    "studyTitle": "Assistant IA pour vous aider √† √©tudier",
    "studyModalTitle": "Parlez avec KnowYOU",
    "greeting": "Bonjour! Je suis KnowYOU",
    "greetingDesc": "Votre assistant sp√©cialis√© en sant√©. Comment puis-je vous aider aujourd'hui?",
    "placeholder": "Tapez votre message sur la sant√©...",
    "placeholderHealth": "Ici vous pouvez rechercher des informations sur la sant√©, l'administration hospitali√®re et l'Hospital Moinhos de Vento.",
    "placeholderStudy": "Ici vous pouvez rechercher divers sujets sur l'IA et sur l'IA KnowYOU qui g√®re cet outil.",
    "placeholderImage": "D√©crivez l'image √©ducative de sant√© que vous souhaitez g√©n√©rer...",
    "placeholderImageStudy": "D√©crivez l'image √©ducative que vous souhaitez g√©n√©rer...",
    "imageLimitHealth": "Images limit√©es au contenu de sant√©",
    "imageLimitStudy": "Images limit√©es √† l'IA et KnowRISK",
    "suggestions": "Suggestions:",
    "imageSuggestions": "Suggestions d'Images:",
    "typing": "√âcriture...",
    "transcribing": "Transcription...",
    "listening": "√âcoute... (parlez naturellement)",
    "waiting": "En Attente...",
    "processing": "Traitement...",
    "voiceTimeout": "La dict√©e se termine apr√®s 5 secondes de silence",
    "generatingImage": "G√©n√©ration d'image...",
    "generatingAudio": "G√©n√©ration d'audio...",
    "thinking": "R√©flexion...",
    "clear": "Effacer",
    "speechNotAvailable": "Reconnaissance vocale non disponible",
    "speechFallback": "Utilisation d'une m√©thode de transcription alternative.",
    "micError": "Erreur lors de l'activation du microphone",
    "micPermissions": "V√©rifiez les autorisations de votre navigateur.",
    "transcriptionError": "Erreur de transcription",
    "transcriptionRetry": "Impossible de transcrire l'audio. Veuillez r√©essayer.",
    "imageRejected": "Image non autoris√©e",
    "imageGuardrailHealth": "Seules les images li√©es √† la sant√© sont autoris√©es dans ce chat.",
    "imageGuardrailStudy": "Seules les images li√©es √† l'IA et KnowRISK sont autoris√©es dans ce chat.",
    "imageGenerationError": "Impossible de g√©n√©rer l'image. Veuillez r√©essayer.",
    "copied": "Texte copi√©!",
    "copyFailed": "√âchec de la copie",
    "miniPlayer": {
      "stop": "Arr√™ter",
      "close": "Fermer"
    }
  },
  "audio": {
    "play": "Lire",
    "pause": "Pause",
    "stop": "Arr√™ter",
    "generating": "G√©n√©ration de l'audio...",
    "download": "T√©l√©charger",
    "downloadImage": "T√©l√©charger l'image",
    "copy": "Copier",
    "notAvailable": "Audio non disponible",
    "playError": "Erreur lors de la lecture de l'audio"
  },
  "turingLegacy": {
    "title": "L'H√©ritage de Turing",
    "quote": "Bien avant que nous parlions aux ordinateurs, il a os√© demander s'ils pouvaient penser. D√©chiffrer Enigma a gagn√© la guerre, mais le 'Jeu de l'imitation' a gagn√© le temps.",
    "author": "par Fernando Arbache"
  },
  "footer": {
    "tagline": "R√©volutionner la Requalification √† l'√àre de l'IA",
    "copyright": "¬© 2024 KnowYOU par KnowRisk. Tous droits r√©serv√©s."
  },
  "floatingButton": {
    "tooltip": "Parler avec KnowYOU"
  },
  "documentAttach": {
    "title": "üìé Joindre un Document de Sant√©",
    "loading": "Chargement...",
    "noDocuments": "Aucun document de sant√© disponible",
    "noDocumentsDesc": "T√©l√©chargez des documents dans le panneau d'administration",
    "chunks": "morceaux",
    "readable": "‚úì Lisible",
    "attached": "Document Joint",
    "attachedDesc": "Le document \"{{documentName}}\" a √©t√© joint √† la conversation.",
    "removed": "Document Supprim√©",
    "removedDesc": "Le contexte du document a √©t√© supprim√© de la conversation.",
    "disclaimerTitle": "‚ö†Ô∏è Avertissement - Nouveau Contenu Ajout√©",
    "disclaimerMessage": "Le document \"{{documentName}}\" a √©t√© joint au contexte de cette conversation. Les r√©ponses incluent maintenant des informations sp√©cifiques de ce document. Cliquez sur X pour supprimer.",
    "removeButton": "Supprimer le document"
  },
  "documentRouting": {
    "title": "üìÑ Le Document N√©cessite une D√©finition de Destination",
    "description": "Le document \"{{filename}}\" a √©t√© class√© comme G√©n√©ral. Choisissez o√π l'appliquer pour activer les garde-fous appropri√©s:",
    "suggestedTags": "Tags sugg√©r√©s:",
    "health": "Sant√©",
    "healthDesc": "Sant√© et m√©decine",
    "study": "√âtude",
    "studyDesc": "KnowRISK et ACC",
    "general": "G√©n√©ral",
    "generalDesc": "Garder tel quel"
  },
  "admin": {
    "tooltips": {
      "tts": {
        "title": "TTS, Phon√©tique et Intelligence Artificielle",
        "intro": "signifie **Text-to-Speech** (Texte vers Parole). Dans le contexte de l'Intelligence Artificielle et de la phon√©tique, il fait r√©f√©rence √† la technologie qui convertit le texte √©crit en audio parl√© synth√©tique, simulant la voix humaine.",
        "description": "Il ne s'agit pas seulement de \"lire\" des mots, mais de comprendre comment ces mots doivent sonner en fonction des r√®gles linguistiques et des mod√®les d'apprentissage profond.",
        "pipeline": {
          "title": "1. Le Flux du Processus (Pipeline)",
          "intro": "Pour qu'une IA transforme le texte en parole, elle passe par des √©tapes critiques o√π la phon√©tique est essentielle :",
          "step1": "**Normalisation du Texte :** L'IA convertit les nombres, abr√©viations et symboles en mots complets.",
          "step1Example": "*Exemple :* \"10,00 ‚Ç¨\" devient \"dix euros\".",
          "step2": "**Conversion Graph√®me-Phon√®me (G2P) :** C'est l√† que la magie de la phon√©tique op√®re. Le syst√®me traduit l'√©criture (graph√®mes) en repr√©sentation sonore (phon√®mes).",
          "step2Example": "*Exemple :* Le mot \"maison\" est converti en quelque chose comme `/m/ /…õ/ /z/ /…îÃÉ/`.",
          "step3": "**Analyse Prosodique :** L'IA d√©termine le rythme, l'intonation (question vs. affirmation) et l'emphase (accentuation).",
          "step4": "**Synth√®se Audio :** L'ordinateur g√©n√®re les ondes sonores finales bas√©es sur ces instructions."
        },
        "phoneticsRole": {
          "title": "2. Le R√¥le de la Phon√©tique dans l'IA",
          "intro": "La phon√©tique dans le TTS moderne n'est pas seulement un dictionnaire de prononciation ; c'est un syst√®me d'interpr√©tation dynamique.",
          "disambiguation": "**D√©sambigu√Øsation des Homographes :** L'IA doit analyser le contexte pour savoir quel phon√®me utiliser pour des mots qui s'√©crivent pareil mais se prononcent diff√©remment.",
          "disambiguationExample": "*Exemple :* \"Je **vis** √† Paris\" (je habite) vs. \"Je **vis** le film\" (j'ai regard√©).",
          "coarticulation": "**Coarticulation :** Dans la parole humaine, le son d'une lettre est influenc√© par les lettres voisines. Un bon TTS utilise l'IA pour lisser ces transitions, √©vitant que la parole sonne \"robotique\" ou hach√©e.",
          "prosody": "**Prosodie et √âmotion :** Les mod√®les neuronaux modernes (comme ceux d'OpenAI ou ElevenLabs) analysent la s√©mantique du texte pour appliquer l'√©motion correcte (tristesse, excitation, doute) dans la phon√©tique de la phrase."
        },
        "comparison": {
          "title": "3. √âvolution : TTS Param√©trique vs. TTS Neural",
          "characteristic": "Caract√©ristique",
          "traditional": "TTS Traditionnel",
          "neural": "TTS Neural",
          "method": "M√©thode",
          "methodTraditional": "\"Colle\" des morceaux d'audio enregistr√©s.",
          "methodNeural": "G√©n√®re des ondes sonores √† partir de z√©ro via r√©seaux neuronaux.",
          "naturalness": "Naturel",
          "naturalnessTraditional": "Sonne robotique, avec des d√©fauts de jonction.",
          "naturalnessNeural": "Presque impossible √† distinguer de la voix humaine.",
          "phonetics": "Phon√©tique",
          "phoneticsTraditional": "Bas√©e strictement sur des r√®gles fixes.",
          "phoneticsNeural": "Apprend des motifs phon√©tiques complexes.",
          "flexibility": "Flexibilit√©",
          "flexibilityTraditional": "Difficile de changer le style de voix.",
          "flexibilityNeural": "Peut cloner des voix ou changer de styles facilement."
        },
        "technologies": {
          "title": "4. Technologies Principales",
          "intro": "Aujourd'hui, les mod√®les TTS les plus avanc√©s utilisent des architectures telles que :",
          "tacotron": "**Tacotron 2 :** Mappe le texte directement vers des spectrogrammes (repr√©sentations visuelles du son).",
          "wavenet": "**WaveNet / WaveGlow :** G√©n√®rent des ondes audio brutes point par point, r√©sultant en tr√®s haute fid√©lit√©.",
          "valle": "**VALL-E / XTTS :** Mod√®les capables de cloner la voix et l'acoustique de la pi√®ce avec quelques secondes d'audio de r√©f√©rence."
        }
      },
      "rag": {
        "title": "RAG : G√©n√©ration Augment√©e par R√©cup√©ration",
        "intro": "**RAG** signifie **Retrieval-Augmented Generation** (G√©n√©ration Augment√©e par R√©cup√©ration). C'est une architecture d'IA qui combine la r√©cup√©ration d'informations avec la g√©n√©ration de texte, permettant aux mod√®les de langage de r√©pondre en se basant sur des documents sp√©cifiques.",
        "whatIs": {
          "title": "1. Qu'est-ce que RAG ?",
          "intro": "RAG est un paradigme hybride qui unit deux mondes :",
          "retrieval": "**R√©cup√©ration (Retrieval) :** Un syst√®me de recherche s√©mantique trouve les documents les plus pertinents pour la question de l'utilisateur.",
          "generation": "**G√©n√©ration Augment√©e :** Le LLM re√ßoit ces documents comme contexte additionnel pour fonder sa r√©ponse.",
          "result": "*R√©sultat :* Des r√©ponses pr√©cises, √† jour et fond√©es sur des sources v√©rifiables."
        },
        "comparison": {
          "title": "2. Pourquoi RAG est Sup√©rieur au LLM Pur ?",
          "aspect": "Aspect",
          "traditional": "LLM Traditionnel",
          "rag": "RAG",
          "knowledge": "Connaissance",
          "knowledgeTraditional": "Limit√©e √† l'entra√Ænement (date limite)",
          "knowledgeRag": "Mise √† jour dynamique avec nouveaux documents",
          "precision": "Pr√©cision",
          "precisionTraditional": "Peut \"halluciner\" des informations",
          "precisionRag": "Fond√© sur des sources v√©rifiables",
          "traceability": "Tra√ßabilit√©",
          "traceabilityTraditional": "Impossible de citer des sources",
          "traceabilityRag": "Peut r√©f√©rencer des documents sp√©cifiques",
          "customization": "Personnalisation",
          "customizationTraditional": "N√©cessite un fine-tuning co√ªteux",
          "customizationRag": "Il suffit d'ajouter des documents √† l'index"
        },
        "pipeline": {
          "title": "3. Le Pipeline RAG Impl√©ment√©",
          "intro": "Le syst√®me utilise un pipeline en 4 √©tapes :",
          "step1": "**Ingestion (ETL) :** Les documents sont trait√©s, valid√©s et fragment√©s en chunks optimis√©s.",
          "step2": "**Indexation Vectorielle :** Chaque chunk est converti en embedding et stock√© avec pgvector.",
          "step3": "**Recherche Hybride :** Combine similarit√© s√©mantique + filtres de m√©tadonn√©es (tags, chat_type).",
          "step4": "**G√©n√©ration Fond√©e :** Le LLM re√ßoit les chunks pertinents comme contexte obligatoire."
        },
        "technical": {
          "title": "4. Composants Techniques",
          "embeddings": "**Embeddings :** OpenAI text-embedding-3-small (1536 dimensions)",
          "vectorStore": "**Vector Store :** PostgreSQL + pgvector (recherche par similarit√© cosinus)",
          "chunking": "**Chunking :** 750 mots avec 180 de chevauchement",
          "threshold": "**Seuil :** Similarit√© minimum de 0.15 pour inclusion dans le contexte"
        },
        "glossary": {
          "title": "5. Glossaire des Sigles",
          "rag": "**RAG** - Retrieval-Augmented Generation",
          "etl": "**ETL** - Extract, Transform, Load",
          "llm": "**LLM** - Large Language Model",
          "embedding": "**Embedding** - Repr√©sentation vectorielle de texte",
          "chunk": "**Chunk** - Fragment de document index√©",
          "pgvector": "**pgvector** - Extension PostgreSQL pour vecteurs"
        }
      },
      "embeddings": {
        "title": "Embeddings et Similarit√© Vectorielle",
        "intro": "sont des repr√©sentations num√©riques de texte qui capturent le sens s√©mantique. Elles permettent aux ordinateurs de 'comprendre' et comparer des textes math√©matiquement.",
        "whatIs": {
          "title": "1. Que sont les Embeddings ?",
          "content": "Les embeddings sont des vecteurs de nombres qui repr√©sentent des mots, phrases ou documents dans un espace math√©matique. Les textes avec des significations similaires sont proches dans cet espace, permettant des recherches s√©mantiques."
        },
        "models": {
          "title": "2. Mod√®les d'Embeddings",
          "intro": "Diff√©rents mod√®les g√©n√®rent des embeddings avec des caract√©ristiques distinctes :",
          "model": "Mod√®le",
          "dimensions": "Dimensions",
          "useCase": "Cas d'Usage",
          "smallDim": "1536",
          "smallUse": "Usage g√©n√©ral, bon rapport qualit√©-prix",
          "largeDim": "3072",
          "largeUse": "Haute pr√©cision, textes complexes",
          "adaDim": "1536",
          "adaUse": "H√©rit√©, encore largement utilis√©"
        },
        "similarity": {
          "title": "3. Calcul de Similarit√©",
          "intro": "Nous mesurons la 'proximit√©' de deux embeddings en utilisant :",
          "cosine": "**Similarit√© Cosinus :** Mesure l'angle entre vecteurs (0 = diff√©rents, 1 = identiques). Utilis√© dans le syst√®me.",
          "euclidean": "**Distance Euclidienne :** Distance g√©om√©trique directe entre points.",
          "dot": "**Produit Scalaire :** Consid√®re la magnitude des vecteurs, utile pour les recommandations."
        },
        "quality": {
          "title": "4. Crit√®res de Qualit√©",
          "intro": "Des embeddings de haute qualit√© n√©cessitent :",
          "criterion1": "**Chunks bien structur√©s :** Textes coh√©rents et complets (id√©alement 200-800 mots).",
          "criterion2": "**Contexte pr√©serv√© :** √âviter les coupures au milieu de phrases ou concepts.",
          "criterion3": "**Densit√© s√©mantique :** Chunks avec contenu riche et pertinent, pas seulement des connecteurs."
        },
        "glossary": {
          "title": "5. Glossaire",
          "vector": "**Vecteur** - Liste ordonn√©e de nombres repr√©sentant un concept",
          "dimension": "**Dimension** - Nombre de valeurs dans le vecteur (ex: 1536)",
          "distance": "**Distance** - Mesure de diff√©rence entre deux vecteurs",
          "similarity": "**Similarit√©** - Mesure de proximit√© entre deux vecteurs (inverse de la distance)"
        }
      },
      "chunks": {
        "title": "Chunks et Fragmentation",
        "intro": "sont des fragments de documents optimis√©s pour la recherche s√©mantique. La strat√©gie de fragmentation impacte directement la qualit√© des r√©ponses RAG.",
        "whatIs": {
          "title": "1. Que sont les Chunks ?",
          "content": "Les chunks sont des unit√©s de texte en lesquelles les documents sont divis√©s pour le traitement RAG. Chaque chunk re√ßoit son propre embedding et est index√© ind√©pendamment, permettant une recherche granulaire d'extraits pertinents."
        },
        "strategy": {
          "title": "2. Strat√©gies de Fragmentation",
          "intro": "Il existe diff√©rentes fa√ßons de diviser les documents :",
          "fixedSize": "**Taille Fixe :** Divise par nombre de mots ou tokens (utilis√© dans ce syst√®me : ~750 mots).",
          "semantic": "**S√©mantique :** Divise par paragraphes, sections ou sujets complets.",
          "overlap": "**Chevauchement :** Les chunks partagent des mots aux bordures pour pr√©server le contexte (180 mots de chevauchement)."
        },
        "sizing": {
          "title": "3. Taille Id√©ale des Chunks",
          "intro": "La taille impacte la pr√©cision et le contexte :",
          "size": "Taille",
          "words": "Mots",
          "quality": "Qualit√©",
          "verySmallWords": "< 50",
          "verySmallQuality": "‚ùå Tr√®s faible - contexte insuffisant",
          "smallWords": "50-200",
          "smallQuality": "‚ö†Ô∏è Faible - contexte limit√©",
          "mediumWords": "200-800",
          "mediumQuality": "‚úÖ Bonne - √©quilibre id√©al",
          "largeWords": "800+",
          "largeQuality": "‚ö° Excellent - contexte riche"
        },
        "impact": {
          "title": "4. Impact sur RAG",
          "intro": "Des chunks mal dimensionn√©s affectent :",
          "precision": "**Pr√©cision :** Les petits chunks perdent du contexte ; les grands m√©langent plusieurs sujets.",
          "context": "**Contexte :** Le chevauchement pr√©serve la continuit√© entre chunks adjacents.",
          "performance": "**Performance :** Plus de chunks = plus de recherches, mais plus grande granularit√©."
        },
        "glossary": {
          "title": "5. Glossaire",
          "chunk": "**Chunk** - Fragment de document avec taille optimis√©e pour la recherche",
          "overlap": "**Chevauchement** - Mots partag√©s entre chunks cons√©cutifs",
          "window": "**Fen√™tre** - Taille du chunk en mots ou tokens",
          "token": "**Token** - Unit√© de texte de base (~0.75 mots en portugais)"
        }
      },
      "vectorStore": {
        "title": "Vector Store et pgvector",
        "intro": "est le composant responsable du stockage et de la recherche de repr√©sentations vectorielles (embeddings) de texte. Il permet des recherches par similarit√© s√©mantique plut√¥t que par correspondance exacte de mots.",
        "whatIs": {
          "title": "1. Qu'est-ce qu'un Vector Store ?",
          "content": "Un Vector Store est une base de donn√©es sp√©cialis√©e pour stocker des vecteurs de haute dimension et effectuer des recherches par similarit√©. Chaque fragment de document est converti en un vecteur num√©rique (embedding) qui capture son sens s√©mantique."
        },
        "pgvector": {
          "title": "2. Pourquoi pgvector ?",
          "intro": "pgvector est une extension PostgreSQL qui ajoute un support natif des vecteurs :",
          "feature1": "**Int√©gration Native :** Fonctionne directement dans PostgreSQL existant, sans base de donn√©es s√©par√©e.",
          "feature2": "**Performance :** Supporte les index HNSW et IVFFlat pour des recherches rapides sur des millions de vecteurs.",
          "feature3": "**Simplicit√© :** Combine donn√©es structur√©es (m√©tadonn√©es, tags) avec recherche vectorielle dans la m√™me requ√™te."
        },
        "similarity": {
          "title": "3. M√©triques de Similarit√©",
          "intro": "Le syst√®me supporte diff√©rentes fa√ßons de mesurer la \"proximit√©\" de deux vecteurs :",
          "metric": "M√©trique",
          "description": "Description",
          "useCase": "Usage Courant",
          "cosineDesc": "Mesure l'angle entre vecteurs (ignore la magnitude)",
          "cosineUse": "Texte, s√©mantique",
          "euclideanDesc": "Distance g√©om√©trique directe entre points",
          "euclideanUse": "Images, clustering",
          "dotDesc": "Produit scalaire (consid√®re la magnitude)",
          "dotUse": "Recommandations"
        },
        "howItWorks": {
          "title": "4. Comment Fonctionne la Recherche",
          "intro": "Le flux de recherche s√©mantique :",
          "step1": "**Requ√™te ‚Üí Embedding :** La question de l'utilisateur est convertie dans le m√™me espace vectoriel que les documents.",
          "step2": "**Recherche ANN :** Approximate Nearest Neighbor trouve les vecteurs les plus proches sans parcourir tous.",
          "step3": "**Seuil :** Seuls les vecteurs avec similarit√© sup√©rieure √† 0.15 sont consid√©r√©s pertinents.",
          "step4": "**Reranking :** Les r√©sultats sont tri√©s et les top-K sont envoy√©s au LLM."
        },
        "glossary": {
          "title": "5. Glossaire Technique",
          "vector": "**Vecteur** - Tableau de nombres repr√©sentant un concept dans l'espace s√©mantique",
          "embedding": "**Embedding** - Vecteur g√©n√©r√© par mod√®le d'IA (ex: text-embedding-3-small)",
          "dimension": "**Dimension** - Nombre de valeurs dans le vecteur (1536 dans ce syst√®me)",
          "index": "**Index HNSW** - Structure de donn√©es pour recherche vectorielle rapide"
        }
      },
      "ragTest": {
        "title": "Test de Recherche RAG",
        "intro": "permet de valider en temps r√©el si une requ√™te sp√©cifique retournerait un contexte pertinent des documents index√©s avant de l'utiliser en production.",
        "whatIs": {
          "title": "1. Qu'est-ce que le Test de Recherche RAG ?",
          "content": "C'est un outil de diagnostic qui simule le processus de r√©cup√©ration de documents du syst√®me RAG. Il permet de tester des requ√™tes et de visualiser quels chunks seraient envoy√©s au LLM comme contexte, avec leurs scores de similarit√© respectifs."
        },
        "howItWorks": {
          "title": "2. Comment Fonctionne le Test",
          "intro": "Le test ex√©cute le flux complet de recherche s√©mantique :",
          "step1": "**Conversion :** La requ√™te est transform√©e en embedding en utilisant le m√™me mod√®le que les documents (text-embedding-3-small).",
          "step2": "**Recherche Vectorielle :** Le syst√®me recherche les chunks les plus similaires dans le Vector Store en utilisant la similarit√© cosinus.",
          "step3": "**Application du Seuil :** Seuls les chunks avec score ‚â• 0.15 sont consid√©r√©s pertinents.",
          "step4": "**Tri :** Les r√©sultats sont tri√©s par similarit√© d√©croissante et les top-K sont affich√©s."
        },
        "interpretation": {
          "title": "3. Interpr√©tation des R√©sultats",
          "intro": "Le test retourne des indicateurs clairs sur la qualit√© de la recherche :",
          "success": "**‚úÖ Contexte Trouv√© :** Le chat utilisera RAG et r√©pondra bas√© sur les documents r√©cup√©r√©s.",
          "failure": "**‚ùå Sans Contexte :** Le chat r√©pondra sans RAG, en utilisant uniquement les connaissances de base du LLM.",
          "score": "**Score de Similarit√© :** Valeurs entre 0.15-0.40 = faible pertinence ; 0.40-0.70 = bonne pertinence ; >0.70 = haute pertinence."
        },
        "useCase": {
          "title": "4. Cas d'Usage",
          "intro": "Utilisez ce test pour :",
          "case1": "**Valider l'Indexation :** V√©rifier si les nouveaux documents ont √©t√© correctement trait√©s et sont recherchables.",
          "case2": "**D√©boguer les √âchecs :** Enqu√™ter pourquoi le chat ne r√©pond pas sur un sujet attendu.",
          "case3": "**Optimiser les Requ√™tes :** Tester diff√©rentes formulations de questions pour trouver la plus efficace."
        },
        "glossary": {
          "title": "5. Glossaire",
          "query": "**Requ√™te** - Question ou terme de recherche test√©",
          "chunk": "**Chunk** - Fragment de document retourn√© par la recherche",
          "similarity": "**Similarit√©** - Score entre 0 et 1 indiquant la pertinence du chunk",
          "threshold": "**Seuil** - Limite minimale de similarit√© (0.15) pour inclusion dans les r√©sultats"
        }
      }
    }
  }
}
